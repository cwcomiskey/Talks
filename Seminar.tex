\documentclass{beamer}

\mode<presentation>
{

  \usetheme{Boadilla}
  \usecolortheme{whale}
  \setbeamercovered{transparent}
  
  \setbeamertemplate{footline}[frame number]{}
  \setbeamertemplate{navigation symbols}{}

}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{mathtools}

\usepackage{natbib}
\bibliographystyle{unsrtnat}

\usepackage{float}
\usepackage{bbm}
\usepackage{mathrsfs}

\usepackage{listings}


\title{Take Me Out to (Analyze) the Ballgame}
\subtitle{Visualization and Analysis Techniques for Big Spatial Data}
\author{Chris Comiskey} 
\institute{Oregon State University} 
\date{\today} 

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% \begin{frame}{Road map}
% \begin{enumerate}
% \addtolength{\itemsep}{0.5\baselineskip}
% \item Variable-resolution heat maps
% \item Shiny confidence intervals
% \item Optimizing in Stan
% \item Predictive Process Models
% \end{enumerate}
% 
% \end{frame}

\begin{frame}{Baseball, Baseball, Baseball} % ==================
\begin{columns}
\column{0.5\textwidth}
\begin{itemize}
\item Chris, rookie year
\end{itemize}

        \begin{figure}[H]
      	\centering
      	\includegraphics[scale=.65]{Images/CWC_Royals.pdf}
      	% Images only work with: setwd("~/Desktop/ResearchRepo/Seminar")
      	\end{figure}

\column{0.5\textwidth}


\end{columns}
\end{frame}

\begin{frame}{Baseball, Baseball, Baseball} % ==================
\begin{columns}
\column{0.5\textwidth}
\begin{itemize}
\item Chris, rookie year
\end{itemize}

        \begin{figure}[H]
      	\centering
      	\includegraphics[scale=.65]{Images/CWC_Royals.pdf}
      	% Images only work with: setwd("~/Desktop/ResearchRepo/Seminar")
      	\end{figure}

\column{0.5\textwidth}
\begin{itemize}
\item Chris, Boston Red Sox
\end{itemize}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.3]{Images/RedSox.jpg}
	\end{figure}

\end{columns}
\end{frame}

\begin{frame}{Hitting Analytics, Then and Now} % ==================
\begin{columns}
\column{0.5\textwidth}
\begin{itemize}
\item $\text{``The Science of Hitting''}_{\text{1970}}$
\end{itemize}

        \begin{figure}[H]
      	\centering
      	\includegraphics[scale=.22]{Images/Williams.jpg}
      	% Images only work with: setwd("~/Desktop/ResearchRepo/Seminar")
      	\end{figure}
      
\begin{itemize}
\item Conceptual breakthrough
\item No data
\end{itemize}

\column{0.5\textwidth}

\end{columns}
\end{frame}

\begin{frame}{Hitting Analytics, Then and Now} % ==================
\begin{columns}
\column{0.5\textwidth}
\begin{itemize}
\item $\text{``The Science of Hitting''}_{\text{1970}}$
\end{itemize}

        \begin{figure}[H]
      	\centering
      	\includegraphics[scale=.22]{Images/Williams.jpg}
      	% Images only work with: setwd("~/Desktop/ResearchRepo/Seminar")
      	\end{figure}
      
\begin{itemize}
\item Iconic breakthrough, hitter
\item No data
\end{itemize}

\column{0.5\textwidth}
\begin{itemize}
\item Hall of Fame exhibit
\end{itemize}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.049]{Images/HoF.jpg}
	\end{figure}
\begin{itemize}
\item Baseball fanatic statistician
\item PITCHf/x data!
\end{itemize}

\end{columns}
\end{frame}

\begin{frame}{Hitting Analytics, Then and Now} % ==================
\begin{columns}
\column{0.5\textwidth}
\begin{itemize}
\item $\text{``The Science of Hitting''}_{\text{1970}}$
\end{itemize}

        \begin{figure}[H]
      	\centering
      	\includegraphics[scale=.22]{Images/Williams.jpg}
      	% Images only work with: setwd("~/Desktop/ResearchRepo/Seminar")
      	\end{figure}
      
\begin{itemize}
\item Iconic breakthrough, hitter
\item No data
\end{itemize}

\column{0.5\textwidth}
\begin{itemize}
\item The Statistics of Hitting
\end{itemize}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.22]{Images/WilliamsMother.jpg}
	\end{figure}
\begin{itemize}
\item PITCHf/x data!
\item Relevant? 
\end{itemize}

\end{columns}
\end{frame}



\begin{frame}{Relevant Research}{} % ======
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
  \item Each MLB\textsuperscript{\textregistered} team spends $\approx$ \$15 million/year
  \item Each MLB\textsuperscript{\textregistered} team employs $\approx$ five quantitative analysts
  \item Heat maps popular on TV broadcasts
        \begin{itemize}
        \addtolength{\itemsep}{0.5\baselineskip}
        \item ESPN\textsuperscript{\textregistered} and MLB\textsuperscript{\textregistered} signed \$700 million/year contract
        \end{itemize}
  \item Joey Votto (\$22.5 mil/year) packs dog-eared copy of ``The Science of Hitting'' 
\item ``Big Data is Changing Baseball'' -  1 TB/game \citep{Delgado2014} 

\end{itemize}
\end{frame}
  
% \begin{frame}{Outline} % ======
%   \tableofcontents
% \end{frame}

\section{The Data} % =======================================

\begin{frame}{The Data} % ======
\begin{itemize}
  \addtolength{\itemsep}{0.5\baselineskip}
\item PITCHf/x\textsuperscript{\textregistered} - Sportsvision, high speed stereoscopic cameras, MLB Advanced Media, Gameday website, *open source*
    \begin{itemize}
    \addtolength{\itemsep}{0.5\baselineskip}
    \item http://www.sportvision.com/media/espn-k-zone
    \end{itemize}
\item MySQL - relational database management system, ~2 GB, `at bat' table 1,711,211 $\times$ 15, table joins
\item Variables   
  \begin{itemize}
   \item \textbf{px} - horizontal location
   \item \textbf{pz} - vertical location
   \item \textbf{des} - pitch outcome
   \item \textbf{ab\_id} - ab bat ID number
   \item \textbf{pitch\_id} - pitch ID number
   \item \textbf{pitch\_type} - fastball, curve ball, etc.
   \item \textbf{stand} - batter handedness
   \item \textbf{batter} - batter ID number
   \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Empirical Success Probability Heat Map} % ============
\begin{columns}

\column{0.5\textwidth}

  \begin{figure}[H]
	\centering
	\includegraphics[scale=.275]{Images/StrikeZone.jpg}
	\end{figure}

\column{0.5\textwidth}
%   \begin{figure}[H]
% 	\centering
% 	\includegraphics[scale=.275]{Images/Mothership.pdf}
%	  \end{figure}
	
\end{columns}
\end{frame}

\begin{frame}{Empirical Success Probability Heat Map} % =============
\begin{columns}

\column{0.5\textwidth}

  \begin{figure}[H]
	\centering
	\includegraphics[scale=.275]{Images/StrikeZone.jpg}
	\end{figure}

\column{0.5\textwidth}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.275]{Images/Mothership.pdf}
	\end{figure}
	
\end{columns}
\end{frame}

\begin{frame}{Empirical Success Probability Heat Map} % =============
\begin{columns}

\column{0.5\textwidth}

\begin{itemize}
\item Grid vertical strike zone face
\item Swing $i$, grid box $b$
\item {\bf Every swing - Bernoulli trial}
  \begin{itemize}
  \item $Y_{i}$ = 1, for swing success
  \end{itemize}
\item $n_{b}$ = number of box b swings
\item box $b$ success probability:
$$\hat{p}_{b} = \frac{1}{n_{b}} \sum_{i \in b} Y_{i}$$ 
\item Map $\hat{p}_{b}$ to box color
\end{itemize}

\column{0.5\textwidth}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.275]{Images/Mothership.pdf}
	\end{figure}
Note: $\hat{p}_{b}$ = {\bf success probability}, not batting average
\end{columns}
\end{frame}

\begin{frame}{Heat Map Resolution, Jhonny Peralta}{Decisions, Decisions...}

  \begin{figure}[H]
	% \centering
	\includegraphics[scale=.28]{Images/Chapter_VarRes.png}
	\end{figure}

\end{frame}

\begin{frame}{Heat Map Resolution, Jhonny Peralta}{Starting from Scratch}
  \begin{figure}[H]
	% \centering
	\includegraphics[scale=.35]{Images/Chapter1x1.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Heat Map Resolution, Jhonny Peralta}{Decisions, Decsions...}
  \begin{figure}[H]
	% \centering
	\includegraphics[scale=.35]{Images/Chapter2x2.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Heat Map Resolution, Jhonny Peralta}{Decisions, Decsions...}
  \begin{figure}[H]
	% \centering
	\includegraphics[scale=.35]{Images/Chapter4x4.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Heat Map Resolution, Jhonny Peralta}{Decisions, Decsions...}
  \begin{figure}[H]
	% \centering
	\includegraphics[scale=.35]{Images/Chapter8x8_100.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Heat Map Resolution, Jhonny Peralta}{Decisions, Decsions...}
  \begin{figure}[H]
	% \centering
	\includegraphics[scale=.35]{Images/Chapter16x16_100.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Heat Map Resolution, Jhonny Peralta}{Decisions, Decsions...}
  \begin{figure}[H]
	% \centering
	\includegraphics[scale=.35]{Images/Chapter32x32_100.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Best of Both (Spatial) Worlds}{}
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item Location specific resolution, rather than one global resolution
\item Stopping rule $\iff$ {\bf sample size} threshold
\item Local resolution conveys data density
  \begin{itemize}
  \addtolength{\itemsep}{0.5\baselineskip}
  \item Big box $\iff$ low density
  \item Bigger box, bigger variance*
  \end{itemize}
\item Choose subdivision algorithm
\end{itemize}
\end{frame}

\begin{frame}{Empirical $\rightarrow$ Model}{}
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item Seek smooth(er) surface
\item Models show relationships
\item Models help understanding
\item Modeling is fun!
\end{itemize}
\end{frame}


\begin{frame}{Let's Model!}{} % ======
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item Hitter, pitcher, umpire
\item Location, pitch type, previous pitch type/location 
\item Game state; count, baserunners
\item Ballpark, day/night, weather
\end{itemize}

{\bf Simplifying assumption}: success probabilities depend only on location $(i)$ and hitter $(j)$: \\

\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item $Y_{ij} \sim \text{Bernoulli}(p_{ij})$
\item $\text{E}[Y_{ij}] = p_{ij}$
\item logit$(p_{ij}) = \pmb{X}_{ij} \pmb{\beta}$
    \begin{itemize}
    \item  covariate vector $\pmb{X}_{ij}$, parameter vector $\pmb{\beta}$
    \end{itemize}
\item Covariates? Biomechanics.
\end{itemize}
\end{frame}

\begin{frame}{Sport Biomechanics}{Golf and Tennis} % ======

\begin{itemize}
\item Rotational movement
\item Change impact point $\implies$ biomechanical
adjustments
\item Adjustments affect impact conditions, outcome probabilities
\end{itemize}

  \begin{figure}[H]
	\centering
	\includegraphics[scale=.4]{Images/Snead.jpg}
	\includegraphics[scale=.25]{Images/golf.jpg}
	\includegraphics[scale=.135]{Images/tennis.jpg}

	\end{figure}

\end{frame}

\begin{frame}{Baseball Biomechanics}{} % ======

\begin{columns}
\column{0.5\textwidth}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.2]{Images/WilliamsPolar.jpg}
	\end{figure}
	
\column{0.5\textwidth}
\begin{itemize}
\item Polar coordinates
\item Translate origin 
\item Radius and angle biomechanically meaningful
\item Big challenge: new origin location
  \begin{itemize}
  \item Fleisig, ASMI
  \item Dowling, Motus
  \end{itemize}
\item Let's model Jhonny Peralta, Mr. 9172 
\end{itemize}

\end{columns}
\end{frame}

\begin{frame}{Logistic Regression Model, Jhonny Peralta} % ======
\begin{itemize}
\item $ \text{logit}(p_{ij}) = \pmb{X}_{ij} \pmb{\beta}$
  \begin{itemize}
  \item swing $i$, hitter $j = $ Jhonny Peralta
  \end{itemize}
\end{itemize}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.24]{Images/Perralta_emp.pdf}
	\includegraphics[scale=.25]{Images/Perralta_fit.pdf}
	\end{figure}

\end{frame}

\begin{frame}{Logistic Regression Model, Jhonny Peralta} % ======
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.215]{Images/Perralta_emp.pdf}
	\includegraphics[scale=.22]{Images/Perralta_fit.pdf}
	\end{figure}
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item Hosmer-Lemeshow (logistic regression) GOF test 
  \begin{itemize}
  \addtolength{\itemsep}{0.5\baselineskip}
  \item Like Pearson $\chi^{2}$ test, but cont. covariates (no categories)
  \item $H_0$: Well fit \\ $H_A$: Lack of fit
  \item p-value = 0.8217
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Logistic Regression Model}{What about a confidence interval?} % ======
\begin{itemize}

\item $ \text{logit}(p_{ij}) = \pmb{X}_{ij} \pmb{\beta}$
  \begin{itemize}
  \item swing $i$, hitter $j$
  \end{itemize}
\end{itemize}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.25]{Images/Perralta_emp.pdf}
	\includegraphics[scale=.26]{Images/Perralta_fit.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Logistic Regression Model}{The State of the Art} % ======
\begin{itemize}

\item $ \text{logit}(p_{ij}) = \pmb{X}_{ij} \pmb{\beta}$
\end{itemize}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.16]{Images/Lower.pdf}
	\includegraphics[scale=.21]{Images/Perralta_fit.pdf}
	\includegraphics[scale=.16]{Images/Upper.pdf}
	\end{figure}
\begin{itemize}
\item Think about 1D and 2D CIs
\end{itemize}
\center{Meet SHINY!!!}
\end{frame}

\begin{frame}{Spatial Logistic Regression Model}{What about a spatial random effect?} % ======
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item $ \text{logit}(p_{ij}) = \pmb{X}_{ij} \pmb{\beta} + \textcolor{blue}{w_{ij}}$
\item Missing covariates
\item Tobler's First Law of Geography
\item Unexplained {\bf spatial} variation in the mean
\end{itemize}
\center{$\rightarrow$ Spatial Generalized Linear Mixed Model (SGLMM)}
\end{frame}

\begin{frame}{Spatial Generalized Linear Mixed Models}{} % ======
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}

\item SGLMM: $\text{logit}(p_{j}) = \pmb{X}_{ij} \pmb{\beta} + w_{ij}$
\item Gaussian Random Field (GRF)
  \begin{itemize}
  \item $\pmb{w} | \pmb{\theta} \sim MVN(\pmb{0}, \Sigma(\pmb{\theta}))$
  \end{itemize}

\item Covariance function
  \begin{itemize}
  \addtolength{\itemsep}{0.5\baselineskip}
  \item $ \Sigma(\phi, \sigma^{2})_{i,j} = \sigma^{2} exp(-||\pmb{s}_{i} - \pmb{s}_{j}||/\phi) $

  \item $||s_{i} - s_{j}||$ - Euclidean distance between $\pmb{s}_{i}$ and $\pmb{s}_{j}$
  \item $\sigma^{2}$ - scale parameter
  \item $\phi$ - range parameter.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Challenges} % ===============
$$ \text{logit}(p_{ij}) = \pmb{X}_{ij} \pmb{\beta} + w_{ij}$$
\begin{itemize} 
\addtolength{\itemsep}{0.5\baselineskip}
\item We do not observe $p_{ij}$, instead Bernoulli$(p_{ij})$ trial outcome
\item We do not observe $n$ latent $w_{ij}$
\item $w_{ij}$ has a correlation structure, unknown correlation parameters $\rightarrow$ $p_{ij}$ has complicated correlation structure

\item Classical methods infeasible
  \begin{itemize}
  \item Imagine writing, maximizing the likelihood, partial derivatives
  \end{itemize}
\end{itemize}

  \begin{figure}[H]
	\centering
	\includegraphics[scale=.2]{Images/THOMAS_BAYES.jpg}
	\end{figure}

\end{frame}

\begin{frame}{Bayesian Hierarchical Model}{} % ============

$$f(\theta|Y) \propto f(Y|\theta)f(\theta)$$

\begin{itemize}
% \textcolor{blue}{f(\pmb{\alpha})} $
\addtolength{\itemsep}{0.5\baselineskip}
\item $Y_{ij}|p_{ij} \sim \text{Bernoulli}(p_{ij})$
\item $\text{logit}(p_{ij}) = \pmb{X}_{ij} \pmb{\beta} + w_{ij}$
  \begin{itemize}
  \addtolength{\itemsep}{0.5\baselineskip}
  \item $\pmb{\beta} \sim f_{\beta}(\pmb{\beta})$
  \item $\pmb{w}_{ij}|(\sigma^{2}, \phi) \sim \text{MVN}(\pmb{0}, \Sigma_{\sigma^{2}, \phi})$
  \item $\sigma \sim f_{\sigma}$, $\phi \sim f_{\phi}$
  \end{itemize}        
\item $f(\pmb{\beta}_{j}, \sigma, \phi|Y_{ij}) \propto  f(Y_{ij}|\pmb{\beta}_{j}, w) \cdot f_{\beta}(\pmb{\beta}_{j}) \cdot f(w|\sigma, \phi) \cdot f_{\sigma}(\sigma) \cdot f_{\phi}(\phi)$
\end{itemize}

Great structure, now implementation.
\end{frame}

\begin{frame}{Markov Chain Magic}
Markov Chain Monte Carlo (MCMC) \\

        \begin{itemize}
        \addtolength{\itemsep}{0.5\baselineskip}
        \item Most popular Bayesian approach, by far 
        \item Target and update parameters (or groups of) in sequence, procedurally reduced dimension
        % Inference from posteriors of ``...arbitrarily large dimension, essentially by reducing the problem to one of recursively solving a series of lower-dimensional (often unidimensional) problems.''
        \item Produce {\bf samples from posterior}; no closed form
        % ``...work by producing not a closed form for posterior, but a sample of values $\{\theta^{(g)}, g = 1, \dots, G\}$ from this distribution.'' ($G$ = number of draws from posterior) 
        \item It works!
        \item Can take time, converges asymptotically, correlated draws
        \end{itemize}
        
There's just one more thing.
\end{frame}

\begin{frame}{One More Big (N) Problem}

\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item GRF $\pmb{w}_{j}$, recall $n \times n$ correlation matrix 
\item Recall multivariate normal pdf
    \begin{itemize}
    \addtolength{\itemsep}{0.5\baselineskip}
    \item $n \times n$ correlation matrix $\rightarrow$ invert $n \times n$ matrix
    \item Jhony Peralta: 9172 $\times$ 9172!
    \end{itemize}
\item Iterative MCMC approach, invert lots of high dimension matrices
\item Known as ``Big N'' problem
\end{itemize}

How bad can it be? Plus, we have physics, and Stan!!
\end{frame}

\begin{frame}{Hamiltonian Markov Chain (HMC) in Stan}
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item Proposal distribution - key component of MCMC algorithm
\item Stan proposal machinery: HMC, from physics (molecular motion)
  \begin{itemize}
  \addtolength{\itemsep}{0.5\baselineskip}
  \item $H(q(t),p(t)) = U(q(t)) + K(p(t))$
  \item Total energy of a system
  \item t = time
  \item {\bf q = position (variables of interest)}, U = potential energy
  \item p = momentum (auxiliary), K = kinetic energy
  \end{itemize}
\item $H(q,p) = -\text{log}f_{q}(q) + p^{T}\pmb{M}^{-1}p/2$
\item Tidy partial derivatives
\item {\bf Short version}: randomly sample momentum p (auxiliary), calculate new position q  --- that's your Metropolis proposal.
\end{itemize}

Stan will be unstoppable!
\end{frame}

\begin{frame}{Slow, Slow, Slow}{Meet Stan the snail.}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=1]{Images/Snail.jpg}
	\end{figure}
	

\end{frame}

\begin{frame}{Slow, Slow, Slow}{Meet Stan the snail.}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=1]{Images/Snail.jpg}
	\end{figure}

How slow? Prohibitively slow. Think geologic time scales; earth is 4.54 billion years old. 

\end{frame}

\begin{frame}{Optimization}{Give Stan a turbo-boost!}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.2]{Images/FastSnail.jpg}
	\end{figure}
\end{frame}

\begin{frame}{Stan Optimization}{}
\begin{columns}
\column{0.5\textwidth}  
  \begin{figure}[H]
	% \centering
	\includegraphics[scale=.5]{Images/StanManCover.pdf}
	\end{figure}
\begin{itemize}
\item Stan Development Team Members
      \begin{itemize}
      \item Andrew Gelman
      \item Bob Carpenter
      \item Rob Trangucci
      \end{itemize}
\end{itemize}


\column{0.5\textwidth}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.4]{Images/Gelman.jpg}
	\end{figure}
	
\end{columns}
\end{frame}

\begin{frame}[fragile]{Stan Optimization}{}
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item Informative (sharp tailed) prior on exponential covariance length-scale parameter $\phi$ for model identifiability (normal or log-normal)
\item Computing time/convergence - ``Complex models'' such as spatial hierarchical models require proper priors for all $\pmb{\beta}$ coefficients.
\item QR factorization on covariate matrix X; reparameterize, adjust priors
\begin{align*}
\pmb{X} &= \pmb{QR} \\
\pmb{X \beta} &= \pmb{QR \beta} \rightarrow \text{Let } \pmb{\theta} = \pmb{R \beta} \\
\pmb{X \beta} &= \pmb{Q \theta} \\
\pmb{\beta} &= \pmb{R^{-1}\theta}
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Stan Optimization}{}
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item Add noise to the covariance matrix diagonal, ensure numerical positive-definiteness
\item Cholesky decomposition and tactical reparameterization 
    \begin{verbatim}
    L = cholesky_decompose(Sigma);
    Z ~ normal(0, 1);
    Z_mod = L * Z;
    Y ~ bernoulli_logit(Q*theta + Z_mod);
    \end{verbatim}
\item Matrix algebra and vectors, over ‘for loops’ and scalars
\begin{verbatim}
hit ~ bernoulli_logit(X*beta + Z)
\end{verbatim}
is faster than
\begin{verbatim}
for (n in 1:N)
    hit[n] = bernoulli_logit(X[n]*beta[n] + Z[n]);
\end{verbatim}
\end{itemize}
On your mark. Get set...
\end{frame}

\begin{frame}{Go Stan 2.0!}

  \begin{figure}[H]
	\centering
	\includegraphics[scale=.5]{Images/Wheee.jpg}
	\end{figure}

\end{frame}

\begin{frame}{How Slow is Stan 2.0?}

\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item N = 1000: 7 hours 45 mins for 1500 draws
  \begin{itemize}
  \item Note: w/o spatial effect: 6 seconds.
  \end{itemize}
\item N = 2000 overnight, 350 draws
\end{itemize}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.32]{Images/Turtle.png}
	\end{figure}

\end{frame}

\begin{frame}{The ``Big N'' Problem Revisited}
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
  \item SGLMM computational costs
  % \item $\mathcal{O}(n^{3})$ % ($t(n) \leq M \cdot n^{3}$ as $n \rightarrow \infty$)
  \begin{itemize}
  \addtolength{\itemsep}{0.5\baselineskip}
  \item Increase at a rate of $\mathcal{O}(n^{3})$: 
  $$t(n) \leq M \cdot n^{3} \text{ as } n \rightarrow \infty$$   
  \item Spatial random effect covariance matrix $\Sigma$ is $n \times n$
  \item Every iteration requires $\Sigma^{-1}$
  \item Determinant of $\Sigma$
  \end{itemize}
\item Computational bottleneck
\end{itemize}
\end{frame}

\begin{frame}{Persistence}

\begin{columns}

\column{0.5\textwidth}

\begin{center}
\Large Secret 7. You can do it. A PhD is 10\% intelligence and 90\%
persistence.
\end{center}

\column{0.5\textwidth}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.5]{Images/Kearns.png}
	\end{figure}

\end{columns}
\end{frame}

\begin{frame}{Predictive Process Models (PPMs)} {Dimension Reduction}
NCAR workshop, Andrew Finley 

  \begin{itemize}
  \addtolength{\itemsep}{0.5\baselineskip}
  \item MSU Deptartments of Forresty and Geography
  \end{itemize}
  
``Gaussian predictive process models for large spatial data sets'' \citep{Banerjee2008}

\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item Competitive approach with computational advantages
\item Project original process onto lower dimensioned subspace; generate parent process realizations at knots
\item ``Spatial sampling design for parameter estimation of the covariance function'' \citep{Zhu2005}
\end{itemize}
\end{frame}

\begin{frame}{PPM Procedure}{Nuts and Bolts}
\begin{align*}
\text{logit}(p_{ij}|\pmb{s}_{ij}) &= \pmb{X}_{ij}(\pmb{s}_{ij}) \pmb{\beta}_{j} + w(\pmb{s}_{ij}) \\
\pmb{w}(\pmb{s}) | \pmb{\theta} &\sim \text{GRF}(\pmb{0}, \pmb{C}(\pmb{\theta}))
\end{align*}
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item $\pmb{w}(\pmb{s})$ - $n \times 1$ vector of random effects, at locations $\pmb{s}$ 
\item $\pmb{0}$ - $n \times 1$ zero vector 
\item $\pmb{C}(\pmb{\theta})$ - $n \times n$, symmetric, positive-definite covariance matrix
    \begin{itemize}
    \item $\pmb{\theta}$ - covariance parameters
    \end{itemize}
\item $C(\pmb{s}_{i}, \pmb{s}_{j}; \pmb{\theta})$ - covariance of random effects  at locations $\pmb{s}_{i}$ and $\pmb{s}_{j}$
      \begin{itemize}
      \addtolength{\itemsep}{0.5\baselineskip}
      \item $C(\pmb{\theta}) = [C(\pmb{s}_{i}, \pmb{s}_{j}; \pmb{\theta})]_{i,j=1}^{n}$
      \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{PPM Knots}
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item $\pmb{S}^{*} = \{\pmb{s}_{1}^{*}, \dots, \pmb{s}_{m}^{*}\}$ --- $m$ knot locations
  \begin{itemize}
  \item $m < < n$ 
  \end{itemize}
\item $\pmb{w}^{*} = \left[w(\pmb{s}_{i}^{*})\right]_{i=1}^{m}$ -- knot location random effects
\item $\pmb{C}^{*}(\pmb{\theta}) = \left[C(\pmb{s}_{i}^{*}, \pmb{s}_{j}^{*})\right]_{i,j = 1}^{m}$ -- knot covariance matrix
  \begin{itemize}
  \item $m \times m$ --- much smaller dimension
  \end{itemize}
\item $\pmb{w}^{*}|\pmb{\theta} \sim \text{GRF}\{\pmb{0}, \pmb{C}^{*}(\pmb{\theta})\}$ -- $m$-dimensional GRF
\end{itemize}

\end{frame}

\begin{frame}{PPM, Krig $w(\pmb{s}_{0})$}
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item Interpolate $w(\pmb{s}_{0})$ using: (i) $m$ knots, (ii) parent process covariance structure, (iii) kriging procedure
\item  $\tilde{w}(\pmb{s}_{0})$ - interpolated random effect
\begin{align*}
\tilde{w}(\pmb{s}_{0}) &= E[w(\pmb{s}_{0})|\pmb{w}^{*}] \\
&= \pmb{c}^{T}(\pmb{s}_{0};\pmb{\theta}) \cdot \pmb{C}^{*-1}(\pmb{\theta}) \cdot \pmb{w}^{*}
\end{align*}
\item $\pmb{c}(\pmb{s}_{0};\pmb{\theta}) = \left[C(\pmb{s}_{0}, \pmb{s}_{j}^{*}; \pmb{\theta})\right]_{j = 1}^{m}$ --- $m \times 1$ vector, $\text{Cov}(w(\pmb{s}_{0}), \text{ knots})$
\item Spatially varying linear combination
\item Minimize squared error loss function (linear predictors, GRFs) 
\end{itemize}

Now replace $w(\pmb{s})$ with $\tilde{w}(\pmb{s})$.
\end{frame}

\begin{frame}[fragile]{The Predictive Process! $\tilde{w}(\pmb{s})$}
Another Gaussian Random Field
\begin{align*}
\tilde{\pmb{w}}(\pmb{s}) &\sim \text{GRF}\{0, \tilde{C}(\cdot)\} \\
\tilde{C}(\pmb{s}, \pmb{s}'; \pmb{\theta}) &= \pmb{c}^{T}(\pmb{s};\pmb{\theta}) \cdot \textcolor{blue}{\pmb{C}^{*-1}(\pmb{\theta})} \cdot \pmb{c}(\pmb{s}';\pmb{\theta})
\end{align*}
And a predictive process model!
    \begin{equation}
    \text{logit}(p_{ij}|\pmb{s}_{ij}) = \pmb{X}_{ij}(\pmb{s}_{ij}) \pmb{\beta}_{j} + \tilde{w}(\pmb{s})
    \end{equation}
    \begin{itemize}
    \item Note dimension of \textcolor{blue}{covariance matrix}.
    \item Implement in \verb|spBayes| \citep{Finley2007}.
    \item Faster?
    \end{itemize}
\end{frame}

\begin{frame}{PPMs, Plot and Results}

\begin{columns}
\column{0.5\textwidth}

      \begin{itemize}
      \addtolength{\itemsep}{0.5\baselineskip}
      \item n = 500, knots = 97, 10K samples, $\approx$ 4 mins
      \item n = 1000, knots = 97, 10K samples, $\approx$ 6.7 mins 
      \item n = 1000, knots = 49, 30K samples, $\approx$ 7 mins 
      \item n = 3000, knots = 49, 80K samples, $\approx$ 54 mins
%      \item n = 9172, knots = 49, ``80,000 samples completed'' but the dreaded rainbow pinwheel never went away when computer tried to resuscitate in morning. We'll never know...
      \end{itemize}

\column{0.5\textwidth}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.29]{Images/Sample_PPM_fit.pdf}
	\end{figure}

\end{columns}
\end{frame}

\begin{frame}{PPM, Plot and Results}

\begin{columns}
\column{0.5\textwidth}

  \begin{figure}[H]
	\centering
	\includegraphics[scale=.29]{Images/Sample_GLM_fit.pdf}
	\end{figure}
	
\column{0.5\textwidth}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.29]{Images/Sample_PPM_fit.pdf}
	\end{figure}

\end{columns}
\end{frame}

\begin{frame}{Current Work, SPDE-INLA}

\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item What about n = 9172?
\item Approximation method, works well for Bayesian hierarchical models with latent GRF
\item Essentially a two part process for continuous domain
  \begin{itemize}
  \item Part 1: SPDE
  \item Part 2: INLA
  \end{itemize}
\item To n = 9172... and beyond!!
\end{itemize}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.02]{Images/BL.jpg}
	\end{figure}

\end{frame}

\begin{frame}{Approximation Part 1, SPDE}
SPDE - Stochastic partial differential equation \citep{Lindgren2011}
$$ (\kappa^{2} - \Delta)^{\alpha/2}x(\pmb{s}) = \mathcal{W}(\pmb{s})$$
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item Convert GRF to Gaussian {\bf Markov} Random Field (GMRF)
    \begin{itemize}
    \item Sparse precision matrix
    \end{itemize}
\item Finite Element Method: project SPDE onto basis representation    
\item Triangular mesh, defines piecewise linear basis representation
$$ \tilde{x}(\pmb{s}) = \sum_{k} \psi_{k}(\pmb{s})x_{k}$$
                  \begin{itemize}
                  \addtolength{\itemsep}{0.5\baselineskip}
                  \item $\psi_{k}(\pmb{s})$ --- deterministic basis functions
                  \item $x_{k}$ --- weights, explicit and sparse precision matrix
                  \item $\tilde{x}(\pmb{s})$ approximates SPDE solution
                  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{SPDE, A Picture Worth 1000 Equations}

  \begin{figure}[H]
	\centering
	\includegraphics[scale=.3]{Images/PLBF.jpg}
	\end{figure}
	\citep{Simpson2012}
\end{frame}

\begin{frame}{Approximation Part 2, INLA}
INLA - Integrated Nested Laplace Approximation \citep{Rue2009} \\ 

        \begin{enumerate}
        \addtolength{\itemsep}{0.5\baselineskip}
        \item Gaussian approximation: $\tilde{p}(\pmb{w}|\pmb{\theta, y})$
        % (match mode and curvature at mode \citep{Rue2005})
        % \item Posterior mode: $\pmb{w}^{*}(\pmb{\theta}) = \text{argmax}_{w}p(\pmb{w}|\pmb{\theta, y})$
        \item Laplace approximation:
        $$ p(\pmb{\theta} | \pmb{y}) \propto \frac{p(\pmb{\theta, y, w})}{p(\pmb{w}|\pmb{\theta, y})} \Big|_{\pmb{w} = \pmb{w}^{*}(\pmb{\theta})} \approx \frac{p(\pmb{\theta, y, w})}{\tilde{p}(\pmb{w}|\pmb{\theta, y})} \Big|_{\pmb{w} = \pmb{w}^{*}(\pmb{\theta})}$$
        \begin{itemize}
        \item $\pmb{w}^{*}(\pmb{\theta}) = \text{argmax}_{w}\tilde{p}(\pmb{w}|\pmb{\theta, y})$
        \end{itemize}
%        (unnormalized posterior at any point, numerically optimize for mode)
        \item Numerical integration: 
        $ p(\theta_{k} | \pmb{y}) \approx \int \tilde{p}(\pmb{\theta}|\pmb{y}) d\pmb{\theta}_{-k} $
        \item Numerical integration: 
        $ p(w_{j} | \pmb{y}) \approx \int \tilde{p}(w_{j}|\pmb{\theta, y})\tilde{p}(\pmb{\theta}|\pmb{y}) d\pmb{\theta} $
        \end{enumerate}
\end{frame}

\begin{frame}{Current Work, SPDE-INLA}
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item Implement in R-INLA \citep{Lindgren2015}
\item Computational costs - increase at a rate of $\mathcal{O}(n^{3/2})$
\item Trade-offs - speed for bias (Binomial data)
\end{itemize}
\end{frame}

\begin{frame}{Next Steps}
\begin{itemize}
\addtolength{\itemsep}{0.5\baselineskip}
\item Cool SPDE-INLA pictures
\item Model evaluation, scoring rules \citep{Bickel2007}
\item Cross validation study
\item Variable-resolution heat map R package?
\item Dynamic Heat Map CIs -- R package?
\end{itemize}
\end{frame}

\begin{frame}{}
\begin{center}
\Huge{Thanks for listening. \\
Questions?}
\end{center}
  \begin{figure}[H]
	\centering
	\includegraphics[scale=.105]{Images/Alix.jpg}
	\includegraphics[scale=.055]{Images/Ewan.jpg}
	\end{figure}
\end{frame}

\bibliography{Baseball}



\end{document}



